{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# DAV 6150 Module 4: Dimensionality Reduction & Feature Selection\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 3 Assignment Review\n",
    "\n",
    "### Missing Data Values\n",
    "\n",
    "Of the 12795 rows contained within the data set, only 6436 (50.3%) represent “complete cases” wherein we find no missing data values for any variables. Analysis shows that eight of the fourteen predictor variables have missing data values.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "\n",
    "#### Missing STARS Values\n",
    "\n",
    "- More than 26% of STARS values are missing. Is that necessarily a cause for concern? Based on domain knowledge + common sense, are there any potentially plausible explanations for why so many wines lack a STARS value?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do We Have Any Variables With Invalid Data Values?\n",
    "\n",
    "Nine variables representing various chemical composition measures within wines (Chlorides, ResidualSugar, FreeSulfurDioxide, CitricAcid, VolatileAcidity, TotalSulfur, DioxideSulphates, FixedAcidity and Alcohol) are present within the data set.\n",
    "\n",
    "A cursory survey of the typically reported values for each of these nine attributes reveals that their values __should be strictly positive__ and __zero bound__. Furthermore, there are many readily available sources of information regarding the __plausible__ data values for each of these attributes.\n",
    "\n",
    "Some potential sources of domain knowledge for this data set (there are many, many others available !!):\n",
    "\n",
    "- Chlorides: https://www.scielo.br/scielo.php?script=sci_arttext&pid=S0101-20612015000100095\n",
    "\n",
    "\n",
    "- Sulfites: https://www.morethanorganic.com/sulphur-in-the-bottle\n",
    "\n",
    "\n",
    "- PH Value: https://www.winemag.com/2019/06/19/what-is-acidity-in-wine/\n",
    "\n",
    "\n",
    "- Acid Index: https://www.ajevonline.org/content/31/3/265\n",
    "\n",
    "\n",
    "- Residual Sugars: https://winemakermag.com/technique/501-measuring-residual-sugar-techniques#:~:text=Residual%20Sugar%20Concentration,-Residual%20sugar%20concentration&text=Dry%20wines%20are%20typically%20in,the%205.0%E2%80%9315%20percent%20range.\n",
    "\n",
    "\n",
    "- Density: https://chemwine.com/home/why-is-wine-density-important-1-sl6yl/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So How Might We Deal With Missing Data Values?\n",
    "\n",
    "Since we don't have the ability to question the authors of the data set, we must __make reasonable assumptions based on our domain knowledge + expertise as data science practitioners.__\n",
    "\n",
    "\n",
    "#### Missing Data Values\n",
    "\n",
    "- For each variable, decide how best to handle the missing data values, e.g., fill with mean or median? Fill via some sort of predictive model?, etc.\n",
    "\n",
    "\n",
    "- Develop a __reasonable__ approach for each variable and explain your rationale __in writing__. Then, ask yourself if your rationale is likely to survive strict scrutiny from your peers and/or colleagues. If not, rethink your approach.\n",
    "\n",
    "\n",
    "Some possible approaches suggested by students:\n",
    "\n",
    "- Use of mean, median or mode values. __Be careful__: Adding too many instances of a single data value can introduce significant bias into the distribution of the attribute that otherwise wouldn't occur.\n",
    "\n",
    "\n",
    "- Imputation via a K-Nearest Neighbors imputation (e.g., https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html). __Be careful__: iterative imputation can be very computationally expensive when applied to large data sets.\n",
    "\n",
    "\n",
    "- Imputation via backfilling and forward filling. __Be careful__: Forward filling and backfilling can be a viable imputation strategy if you are working with __time series__ data. In other contexts, do these approaches make any logical sense or have any sound statistical basis?\n",
    "\n",
    "\n",
    "- Imputation via an iterative imputer (i.e., each feature is imputed sequentially, one after the other, allowing prior imputed values to be used as part of a model in predicting subsequent features. For an example see: https://machinelearningmastery.com/iterative-imputation-for-missing-values-in-machine-learning/ ). __Be careful__: iterative imputation can be very computationally expensive when applied to large data sets.\n",
    "\n",
    "When choosing an imputation method, ask yourself what the strengths and limitations of each of these approaches might be __relative to the data you have at hand__. You need to consider both the __computational complexity__ and the __likely reliability / performance__ of each potential approach when making a decision. For example, for a very small number of missing values it might be appropriate to rely on the use of a median value since imputing the same value for a small number of instances is relatively unlikely to introduce bias within your data (i.e., __alter the probability density function derived from the variable's known data__). \n",
    "\n",
    "For situations in which we have more than just a small number of missing values, use of a more complex imputation approach is generally warranted to increase the likelihood that we will maintain the shape of the probability density function we derived from the variable's known data.\n",
    "\n",
    "Which approach is best / most appropriate? The answer to that question is highly dependent upon your own assessment of the data and its potential use within whatever type of model you hope to construct. __You need to decide upon + be ready to defend your course of action__. That means being ready to explain why you believe your approach will prove to be more effective and/or efficient than other possible approaches + explaining why specific alternative approaches are likely to be less effective / efficient than the approach you have chosen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4\n",
    "\n",
    "\n",
    "## Machine Learning Models: \"Simpler\" is Better\n",
    "\n",
    "- Machine learning models that are overly complex can be difficult, if not impossible, to interpret / explain\n",
    "\n",
    "\n",
    "- Complex models can also be very computationally expensive\n",
    "\n",
    "\n",
    "- A great deal of model complexity can be eliminated by simply reducing the number of explanatory variables employed within a model.\n",
    "\n",
    "\n",
    "- Models can be simplified through the use of __Dimensionality Reduction__ and __Feature Selection__ techniques.\n",
    "\n",
    "\n",
    "## Dimensionality Reduction\n",
    "\n",
    "__What is \"Dimensionality\"?__: Within the context of data science and machine learning, \"dimensionality\" refers to the number of potential explanatory variables (a.k.a., \"features\") contained within a set of data we plan to make use of for the construction of a model.\n",
    "\n",
    "\n",
    "__What is \"The Curse of Dimensionality\"?__: If the number of features available to us within a data set is very large relative to the number of observations, some machine learning algorithms will fail to produce effective models, particularly those which are based on similarity metrics (e.g., K-nearest neighbors, clustering, etc.). \n",
    "\n",
    "\n",
    "__Dimensionality Reduction__: We can reduce the number of __numeric__ features to be used within a model via the application of linear algebra techniques (e.g., Principal Components Analysis (PCA); Singular Value Decomposition (SVD)). These methods allow us to derive orthogonal features from a non-orthogonal collection of __numeric__ features. We can then use these new orthogonal features as explanatory variables within our machine learning models. \n",
    "\n",
    "\n",
    "### Principal Components Analysis (PCA)\n",
    "\n",
    "- PCA uses an orthogonal transformation to convert a set of possibly correlated __numeric__ features into a set of values of linearly uncorrelated (a.k.a., __orthogonal__) features known as __principal components__. These new features are then used in a machine learning model __in place of the original features that were used to generate the principal components__.  The principal components themselves are assessed by ranking them in order of the amount of variance in the data they explain. When used within a machine learning context, we generally opt to select the most relevant principal components (i.e., those that explain the greatest amount of variance). For example, we might decide we want to retain the principal components that cumulatively explain at least x% of the variability in the data. (__NOTE__: it is __NOT__ appropriate to use PCA on categorical features, even when they have been converted to binary \"one-hot\" dummy variables. If you want to reduce the dimensionality of categorical data you should instead rely on the use of __feature selection__ techniques).\n",
    "\n",
    "\n",
    "- Unfortunately, there is no strict rule of thumb to apply when deciding how many principal components to retain for use within a machine learning model. \n",
    "\n",
    "\n",
    "- __A major benefit of PCA__: PCA can often drastically the dimensionality of our data, thereby greatly reducing the number of explanatory variables needed to be included within a machine learning model.\n",
    "\n",
    "\n",
    "- __A major drawback to PCA__: Principal components are extremely difficult to interpret / explain.  As such, any model constructed with principal components will also be __very__ difficult to interpret / explain. \n",
    "\n",
    "\n",
    "- __Another major drawback to PCA__: PCA should not be applied to categorical features, even if they have been converted to binary \"one-hot\" dummy variables.\n",
    "\n",
    "\n",
    "- A good, simple example from your assigned readings: https://towardsdatascience.com/pca-and-svd-explained-with-numpy-5d13b0d2a4d8\n",
    "\n",
    "\n",
    "- Within Python we can make use of the __sklearn.decomposition.PCA()__ function to identify the principal components of a data set: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "\n",
    "Now let's look at a simple example of how to use the output of PCA as input to a machine learning algorithm (adapted from https://stackoverflow.com/questions/32194967/how-to-do-pca-and-svm-for-classification-in-python): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load numpy + 'datasets'. We'll use the \"iris\" data that is provided with sklearn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# load PCA + SVM classifier (\"SVC\") + cross validation functions\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import model_selection\n",
    "\n",
    "# load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# what is the dimensionality of the data?\n",
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, we have 150 observations comprised of 4 features. What do the features look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.1, 3.5, 1.4, 0.2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first row of the array\n",
    "iris.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, each of the features contains floating point numbers. Since all of the features are numeric we can apply PCA to the data set in an attempt to reduce its dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9348581  0.04635375]\n"
     ]
    }
   ],
   "source": [
    "# assign the explanatory variables to a Python object\n",
    "X = iris.data\n",
    "\n",
    "# assign the response variable to a Python object\n",
    "y = iris.target\n",
    "\n",
    "# split the data into training + testing subsets\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "\n",
    "# create an instance of a PCA model +\n",
    "# set the number of components you want to retain\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# apply the PCA function to the training data\n",
    "pca.fit(X_train)\n",
    "\n",
    "# display the explained variance ratio for the principal components we've derived from the data\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application of PCA to the data set has yielded two principal components, the first of which explains 93.485% of the variance in the iris data. The second principal component explains an additional 4.635% of the variance in the data. Therefore, the two principal components explain a total of more than 98% of the variance in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.62747613  4.59320324]\n"
     ]
    }
   ],
   "source": [
    "# display the singular values associated with the 2 principal components\n",
    "print(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# now apply the results of the PCA to the training data to transform it into 2 principal components per observation\n",
    "X_t_train = pca.transform(X_train)\n",
    "\n",
    "# apply the results of the PCA to the testing data to transform it into 2 principal components per observation\n",
    "X_t_test = pca.transform(X_test)\n",
    "\n",
    "# create an instance of an SVM classifier\n",
    "clf = SVC()\n",
    "\n",
    "# fit the SVM classifier to the transformed training data + the response data\n",
    "clf.fit(X_t_train, y_train)\n",
    "\n",
    "# check the accuracy of the SVM classifier using the transformed explanatory variables + the response variable\n",
    "print ('score', clf.score(X_t_test, y_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our SVM classifier has achieved an accuracy score of 93.3% when applied to the two principal components we derived from the iris data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred label [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 1 0 0 1 1 0 2 1 0 1 2 1 0\n",
      " 2 1 1 2 0 2 0 0 1 2 2 2 2 1 2 1 1 2 2 1 2 1 2]\n"
     ]
    }
   ],
   "source": [
    "# if desired, generate predictions from the SVM classifier for the transformed testing data\n",
    "print ('pred label', clf.predict(X_t_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "We can determine which attributes/features to include within a model via the application of a variety of thresholding \"filters\" (e.g., exclude all variables whose variance falls below a certain value; if two or more variables are highly correlated with one another, choose one to use within the model and exclude the others; etc.)\n",
    "\n",
    "Some of the most commonly used feature selection techniques include:\n",
    "\n",
    "__Variance Threshholds__: Start by normalizing the features you plan to use as explanatory variables, then calculate their variances. Features whose values show relatively little variance are much less likely to be introducing valuable information within the context of a model, so they are strong candidates for exclusion from your model. How you choose the threshold value is highly subjective / empirical.\n",
    "\n",
    "\n",
    "__Correlation Thresholds__: Remove features that are highly correlated with other features. As with variance thresholds, the choice of a correlation value to use as a threshold is highly subjectve / empirical.\n",
    "\n",
    "\n",
    "__Forward Selection__: Used in regression modeling. Incrementally add features to a model one at a time until model performance no longer improves. A common approach to Forward Stepwise Search is to begin your modeling using the explanatory variable that is most highly correlated with the response variable, then sequentially add additional explanatory variables in decreasing order of their correlation with the response variable. The general algorithm is as follows:\n",
    "\n",
    "- Start with the null model, a model containing an intercept but no predictors.\n",
    "- Fit a simple linear regression model to each individual explanatory variable and then add to the null model that variable resulting in the lowest residual sum of squares value (\"RSS\").\n",
    "- Add to that model the variable that results in the lowest RSS amongst all two-variable models.\n",
    "- The algorithm continues until some stopping rule is satisfied (i.e. all remaining variables have a p-value greater than some\n",
    "threshold).\n",
    "\n",
    "An example of one way to implement forward selection in Python can be found here: https://towardsdatascience.com/feature-selection-using-wrapper-methods-in-python-f0d352b346f\n",
    "\n",
    "\n",
    "__Backward Selection__: Used in regression modeling. Start your modeling process using all of the explanatory variables you believe to be appropriate, then sequentially remove variables one at a time until model performance starts to substantively degrade. The general algorithm is as follows:\n",
    "\n",
    "- Begin with all variables in the model.\n",
    "- Remove the variable with the largest p-value (i.e. least statistically significant).\n",
    "- The new model is fit, and the variable with the largest p-value is removed.\n",
    "- The algorithm continues until a stopping rule is reached (e.g., the p-values of all variables are <= 0.05)\n",
    "\n",
    "An example of one way to implement backward selection in Python can be found here: https://towardsdatascience.com/feature-selection-using-wrapper-methods-in-python-f0d352b346f\n",
    "\n",
    "\n",
    "__Variance Inflation Factors (VIF)__: A regression-specific metric that can use to help us select features for inclusion within a regression algorithm. A VIF value provides an indication of the presence of multicolinearity amongst the explanatory variables used for regression modeling. VIF is calculated by regressing an explanatory variable against every other available explanatory variable. In general, if your VIF calculations for a feature result in a VIF > 5, that feature is a strong candidate for removal from the model: rerun the model without the variable and check to see whether the model's performance has improved. If so, exclude the feature from the model. If not, leave the feature in the model. An excellent overview of VIF is provided in your assigned reading materials: https://www.statisticshowto.datasciencecentral.com/variance-inflation-factor/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 Assignment Guidelines / Requirements\n",
    "\n",
    "\n",
    "# Final Project Guidelines / Requirements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
